{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1NNAfJ84TAoqPsywoCCZ341mxepwBBfvi",
      "authorship_tag": "ABX9TyNhLezxqEVfRsVmFHZbqiG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharshan4038/BMI-Calculator/blob/main/Ontology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWlartmulGe6"
      },
      "outputs": [],
      "source": [
        "# %pip install PyMuPDF easyocr pandas tabula-py langchain langchain-openai openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install PyPDF2==2.10.5"
      ],
      "metadata": {
        "id": "PDPb1wQPlqtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF for PDF handling\n",
        "import easyocr  # EasyOCR for image text extraction\n",
        "import camelot  # Camelot for table extraction\n",
        "import os"
      ],
      "metadata": {
        "id": "fOz8im2Xmi8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines, tabs, and multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Strip leading and trailing spaces\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "O2VljrzeqkE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = easyocr.Reader(['en'])\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "\n",
        "    # Loop through each page\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "\n",
        "        # Extract text from page\n",
        "        full_text += page.get_text()\n",
        "\n",
        "        # Extract images from the page\n",
        "        image_list = page.get_images(full=True)\n",
        "\n",
        "        for img_index, img in enumerate(image_list):\n",
        "            xref = img[0]  # Get image XREF\n",
        "            base_image = doc.extract_image(xref)  # Extract the image\n",
        "            image_bytes = base_image[\"image\"]\n",
        "            image_ext = base_image[\"ext\"]\n",
        "            image_filename = f\"/content/drive/MyDrive/Ontology/pdf_images/page_{page_num}_img_{img_index}.{image_ext}\"\n",
        "\n",
        "            # Save the image to disk\n",
        "            with open(image_filename, \"wb\") as img_file:\n",
        "                img_file.write(image_bytes)\n",
        "\n",
        "            # Perform OCR on the extracted image\n",
        "            ocr_result = reader.readtext(image_filename, detail=0)\n",
        "            full_text += \"\\n\".join(ocr_result) + \"\\n\"\n",
        "\n",
        "\n",
        "    return full_text\n",
        "\n",
        "pdf_path = \"/content/drive/MyDrive/RAG/test_case.pdf\"\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "pdf_text = clean_text(pdf_text)\n",
        "# print(pdf_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkhHis5hm169",
        "outputId": "9a0fde88-6b9d-43f0-e3eb-51bc6d8c159a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_table(df):\n",
        "    # Clean each column and row by removing unwanted characters\n",
        "    df = df.applymap(lambda x: re.sub(r'\\s+', ' ', str(x)).strip() if isinstance(x, str) else x)\n",
        "    return df"
      ],
      "metadata": {
        "id": "lYPy0BXKrNqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tables_from_pdf(pdf_path):\n",
        "    # Use Camelot to extract tables\n",
        "    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor='stream')\n",
        "\n",
        "    cleaned_tables = []\n",
        "\n",
        "    # Process and clean each table\n",
        "    for i, table in enumerate(tables):\n",
        "        df = table.df  # Extract table as Pandas DataFrame\n",
        "        cleaned_df = clean_table(df)  # Clean the DataFrame\n",
        "        cleaned_tables.append(cleaned_df)\n",
        "\n",
        "        # Optionally save the cleaned table to CSV\n",
        "        cleaned_df.to_csv(f\"/content/drive/MyDrive/Ontology/tables_csv/cleaned_table_{i}.csv\", index=False)\n",
        "\n",
        "    return cleaned_tables\n",
        "\n",
        "pdf_path = \"/content/drive/MyDrive/RAG/test_case.pdf\"\n",
        "cleaned_tables = extract_tables_from_pdf(pdf_path)\n",
        "\n",
        "# # Display cleaned tables\n",
        "# for i, cleaned_table in enumerate(cleaned_tables):\n",
        "#     print(f\"Cleaned Table {i}:\")\n",
        "#     print(cleaned_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mq3HmS0cn6ah",
        "outputId": "e8a58dc0-5fe8-4737-9775-02e697bc4775"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/camelot/parsers/stream.py:365: UserWarning: No tables found in table area 1\n",
            "  warnings.warn(f\"No tables found in table area {table_idx + 1}\")\n",
            "/usr/local/lib/python3.10/dist-packages/camelot/parsers/stream.py:365: UserWarning: No tables found in table area 2\n",
            "  warnings.warn(f\"No tables found in table area {table_idx + 1}\")\n",
            "<ipython-input-28-05ca7958f328>:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: re.sub(r'\\s+', ' ', str(x)).strip() if isinstance(x, str) else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data = {\n",
        "    \"pdf_text\": pdf_text,\n",
        "    \"tables\": cleaned_tables\n",
        "}\n",
        "\n",
        "# # Print the combined cleaned data\n",
        "# print(\"Extracted Text from PDF:\\n\", combined_data['pdf_text'])\n",
        "# for i, table in enumerate(combined_data['tables']):\n",
        "#     # print(f\"\\nCleaned Table {i}:\\n\", table)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SxKXGsmAoN8R"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Agent to generate Ontology"
      ],
      "metadata": {
        "id": "TACJIRUNt58e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import os\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"\"\n",
        "\n",
        "\n",
        "# Initialize the OpenAI LLM for Azure OpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"\",\n",
        "    api_key=\"\",\n",
        "    model = \"gpt-35-turbo\",\n",
        "    max_tokens=1000,\n",
        "    azure_deployment=\"GPT-35-TURBO\",\n",
        "    api_version=\"2024-06-01\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ennse_IdsFOq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define a prompt template for generating the ontology template\n",
        "# ontology_prompt = PromptTemplate(\n",
        "#     input_variables=[\"pdf_text\", \"tables\"],\n",
        "#     template=\"\"\"\n",
        "#       Based on the following extracted content from the PDF, including tables and images,\n",
        "#       identify key entities (classes), relationships, properties, instances. It should contain\n",
        "#       only 5 important classes and its properties and relationships, instances.\n",
        "#       Then, generate an ontology in RDF/Turtle format.\n",
        "\n",
        "#       PDF text: {pdf_text}\n",
        "#       Tables: {tables}\n",
        "\n",
        "#       Please generate only the ontology template without mapping specific data points. I dont want you to give any explanation\n",
        "#       I want only the ontology template.\n",
        "\n",
        "#       eg: RDF/Turtle ontology template:\n",
        "     # # @prefix ex: <http://example.org/ontology#> .\n",
        "#         # Classes\n",
        "#         ex:TestingTool a rdfs:Class .\n",
        "#         ex:Algorithm a rdfs:Class .\n",
        "#         ex:Dataset a rdfs:Class .\n",
        "\n",
        "#         # Properties\n",
        "#         ex:usesDataset a rdf:Property ;\n",
        "#             rdfs:domain ex:TestingTool ;\n",
        "#             rdfs:range ex:Dataset .\n",
        "\n",
        "#         ex:implementsAlgorithm a rdf:Property ;\n",
        "#             rdfs:domain ex:TestingTool ;\n",
        "#             rdfs:range ex:Algorithm .\n",
        "\n",
        "#         # Example Instances\n",
        "#         ex:EvoSuite a ex:TestingTool ;\n",
        "#             ex:implementsAlgorithm ex:GeneticAlgorithm ;\n",
        "#             ex:usesDataset ex:SF100Corpus .\n",
        "\n",
        "#         ex:GeneticAlgorithm a ex:Algorithm .\n",
        "\n",
        "#         ex:SF100Corpus a ex:Dataset ;\n",
        "#             rdfs:label \"SF100 Corpus\" .\n",
        "\n",
        "#             \"\"\"\n",
        "# )\n",
        "\n",
        "# # Create an LLM chain using the prompt and LLM\n",
        "# ontology_chain = ontology_prompt | llm"
      ],
      "metadata": {
        "id": "NkDxS1xDxl6i"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to run the agent and generate ontology\n",
        "# def generate_ontology(cleaned_data):\n",
        "#     pdf_text = cleaned_data['pdf_text'][:5000] #Truncate to the first 5000 characters\n",
        "#     tables = \"\\n\".join([str(table) for table in cleaned_data['tables'][:5]])  # Convert tables to string and use only the first 5\n",
        "\n",
        "#     # Run the RunnableSequence (use invoke, not run)\n",
        "#     ontology_template = ontology_chain.invoke({\n",
        "#         \"pdf_text\": pdf_text,\n",
        "#         \"tables\": tables\n",
        "#     })\n",
        "\n",
        "#     return ontology_template"
      ],
      "metadata": {
        "id": "hNBkdOOoyJJr"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming you have the cleaned data from the PDF stored in 'combined_data'\n",
        "# combined_data = {\n",
        "#     \"pdf_text\": pdf_text,        # Cleaned text from PDF\n",
        "#     \"tables\": cleaned_tables     # Cleaned tables from the PDF\n",
        "# }\n",
        "\n",
        "# # Generate the ontology template using Azure OpenAI\n",
        "# ontology_template = generate_ontology(combined_data)\n",
        "\n",
        "# # Print the generated ontology template\n",
        "# print(ontology_template.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwaZQ7j1ySpc",
        "outputId": "edea939d-b73f-4462-8811-c37338eaa64a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@prefix ex: <http://example.org/ontology#> .\n",
            "\n",
            "# Classes\n",
            "ex:Software a rdfs:Class .\n",
            "ex:TestingTool a rdfs:Class .\n",
            "ex:TestSuite a rdfs:Class .\n",
            "ex:Dataset a rdfs:Class .\n",
            "ex:ResearchPaper a rdfs:Class .\n",
            "\n",
            "# Properties\n",
            "ex:implements a rdf:Property ;\n",
            "    rdfs:domain ex:Software ;\n",
            "    rdfs:range ex:TestingTool .\n",
            "\n",
            "ex:hasTestSuite a rdf:Property ;\n",
            "    rdfs:domain ex:Software ;\n",
            "    rdfs:range ex:TestSuite .\n",
            "\n",
            "ex:usesDataset a rdf:Property ;\n",
            "    rdfs:domain ex:TestingTool ;\n",
            "    rdfs:range ex:Dataset .\n",
            "\n",
            "ex:hasResearchPaper a rdf:Property ;\n",
            "    rdfs:domain ex:Software ;\n",
            "    rdfs:range ex:ResearchPaper .\n",
            "\n",
            "# Example Instances\n",
            "ex:SoftwareTool1 a ex:Software ;\n",
            "    ex:implements ex:TestingTool1 ;\n",
            "    ex:hasTestSuite ex:TestSuite1 ;\n",
            "    ex:usesDataset ex:Dataset1 ;\n",
            "    ex:hasResearchPaper ex:ResearchPaper1 .\n",
            "\n",
            "ex:TestingTool1 a ex:TestingTool .\n",
            "\n",
            "ex:TestSuite1 a ex:TestSuite .\n",
            "\n",
            "ex:Dataset1 a ex:Dataset .\n",
            "\n",
            "ex:ResearchPaper1 a ex:ResearchPaper .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the generated ontology to a .ttl file\n",
        "with open(\"/content/drive/MyDrive/Ontology/Ontology_Template/ontology_template.ttl\", \"w\") as f:\n",
        "    f.write(ontology_template.content)"
      ],
      "metadata": {
        "id": "FjMfdU_RyZ6S"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Groq API"
      ],
      "metadata": {
        "id": "F25eJkUlDfii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"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\""
      ],
      "metadata": {
        "id": "mIhtowG346wD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLHQ60p4DG7J",
        "outputId": "c8dba6d7-b565-4ebe-b9ea-344bc03a450c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.2.38)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (0.1.117)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.0.7)\n",
            "Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.11.0 langchain-groq-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"Gemma2-9b-It\",\n",
        "               groq_api_key=groq_api_key,\n",
        "               temperature=0.3,\n",
        "               max_tokens=8192,\n",
        "               top_p=0.2\n",
        "      )\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDAc-1VEDO18",
        "outputId": "bbd158c8-3f17-48cb-b7af-a0db3255d2f4"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py:355: UserWarning: WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e424cc03340>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e424d3bbd60>, model_name='Gemma2-9b-It', temperature=0.3, model_kwargs={'top_p': 0.2}, groq_api_key=SecretStr('**********'), max_tokens=8192)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for generating the ontology template\n",
        "ontology_prompt = PromptTemplate(\n",
        "    input_variables=[\"pdf_text\", \"tables\"],\n",
        "    template=\"\"\"\n",
        "      Based on the following extracted content from the PDF, including tables and images,\n",
        "      identify key entities (classes), relationships, properties, instances. It should contain\n",
        "      only 5 important classes and its properties and relationships, instances.\n",
        "      Then, generate an ontology in RDF/Turtle format.\n",
        "\n",
        "      PDF text: {pdf_text}\n",
        "      Tables: {tables}\n",
        "\n",
        "      Please generate only the ontology template without mapping specific data points. I dont want you to give any explanation\n",
        "      I want only the ontology template.\n",
        "\n",
        "      eg: RDF/Turtle ontology template:\n",
        "      @prefix ex: <http://example.org/ontology#> .\n",
        "        # Classes\n",
        "        ex:TestingTool a rdfs:Class .\n",
        "        ex:Algorithm a rdfs:Class .\n",
        "        ex:Dataset a rdfs:Class .\n",
        "\n",
        "        # Properties\n",
        "        ex:usesDataset a rdf:Property ;\n",
        "            rdfs:domain ex:TestingTool ;\n",
        "            rdfs:range ex:Dataset .\n",
        "\n",
        "        ex:implementsAlgorithm a rdf:Property ;\n",
        "            rdfs:domain ex:TestingTool ;\n",
        "            rdfs:range ex:Algorithm .\n",
        "\n",
        "        # Example Instances\n",
        "        ex:EvoSuite a ex:TestingTool ;\n",
        "            ex:implementsAlgorithm ex:GeneticAlgorithm ;\n",
        "            ex:usesDataset ex:SF100Corpus .\n",
        "\n",
        "        ex:GeneticAlgorithm a ex:Algorithm .\n",
        "\n",
        "        ex:SF100Corpus a ex:Dataset ;\n",
        "            rdfs:label \"SF100 Corpus\" .\n",
        "\n",
        "            \"\"\"\n",
        ")\n",
        "\n",
        "# Create an LLM chain using the prompt and LLM\n",
        "ontology_chain = ontology_prompt | llm"
      ],
      "metadata": {
        "id": "jsWygdnPDT1B"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run the agent and generate ontology\n",
        "def generate_ontology(cleaned_data):\n",
        "    pdf_text = cleaned_data['pdf_text'][:10000] #Truncate to the first 5000 characters\n",
        "    tables = \"\\n\".join([str(table) for table in cleaned_data['tables'][:5]])  # Convert tables to string and use only the first 5\n",
        "\n",
        "    # Run the RunnableSequence (use invoke, not run)\n",
        "    ontology_template = ontology_chain.invoke({\n",
        "        \"pdf_text\": pdf_text,\n",
        "        \"tables\": tables\n",
        "    })\n",
        "\n",
        "    return ontology_template"
      ],
      "metadata": {
        "id": "qtigLspYDjNI"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the cleaned data from the PDF stored in 'combined_data'\n",
        "combined_data = {\n",
        "    \"pdf_text\": pdf_text,        # Cleaned text from PDF\n",
        "    \"tables\": cleaned_tables     # Cleaned tables from the PDF\n",
        "}\n",
        "\n",
        "# Generate the ontology template using Azure OpenAI\n",
        "ontology_template = generate_ontology(combined_data)\n",
        "\n",
        "# Print the generated ontology template\n",
        "print(ontology_template.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTrZ-PqyDpLk",
        "outputId": "f9a8a104-c8b9-4cdb-95a3-e5e9fbb791ad"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```turtle\n",
            "@prefix ex: <http://example.org/ontology#> .\n",
            "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
            "@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
            "\n",
            "# Classes\n",
            "ex:TestTool a rdfs:Class .\n",
            "ex:TestTechnique a rdfs:Class .\n",
            "ex:Dataset a rdfs:Class .\n",
            "ex:Algorithm a rdfs:Class .\n",
            "ex:ResearchPaper a rdfs:Class .\n",
            "\n",
            "# Properties\n",
            "ex:usesTechnique a rdf:Property ;\n",
            "    rdfs:domain ex:TestTool ;\n",
            "    rdfs:range ex:TestTechnique .\n",
            "\n",
            "ex:basedOn a rdf:Property ;\n",
            "    rdfs:domain ex:ResearchPaper ;\n",
            "    rdfs:range ex:TestTool .\n",
            "\n",
            "ex:employsAlgorithm a rdf:Property ;\n",
            "    rdfs:domain ex:TestTool ;\n",
            "    rdfs:range ex:Algorithm .\n",
            "\n",
            "ex:usesDataset a rdf:Property ;\n",
            "    rdfs:domain ex:TestTool ;\n",
            "    rdfs:range ex:Dataset .\n",
            "\n",
            "ex:publishedIn a rdf:Property ;\n",
            "    rdfs:domain ex:ResearchPaper ;\n",
            "    rdfs:range ex:DigitalLibrary .\n",
            "\n",
            "ex:hasTitle a rdf:Property ;\n",
            "    rdfs:domain ex:ResearchPaper ;\n",
            "    rdfs:range xsd:string .\n",
            "\n",
            "# Example Instances\n",
            "ex:EvoSuite a ex:TestTool ;\n",
            "    ex:usesTechnique ex:GeneticAlgorithm ;\n",
            "    ex:employsAlgorithm ex:GeneticAlgorithm ;\n",
            "    ex:usesDataset ex:SF100Corpus .\n",
            "\n",
            "ex:GeneticAlgorithm a ex:Algorithm .\n",
            "\n",
            "ex:SF100Corpus a ex:Dataset ;\n",
            "    rdfs:label \"SF100 Corpus\" .\n",
            "\n",
            "ex:IEEE a ex:DigitalLibrary ;\n",
            "    rdfs:label \"IEEE\" .\n",
            "\n",
            "ex:ACM a ex:DigitalLibrary ;\n",
            "    rdfs:label \"ACM\" .\n",
            "\n",
            "ex:Paper1 a ex:ResearchPaper ;\n",
            "    ex:publishedIn ex:IEEE ;\n",
            "    ex:hasTitle \"A Novel Test Case Generation Technique\" . \n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixtral Model"
      ],
      "metadata": {
        "id": "y5akEI4aIbOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model = ChatGroq(model=\"mixtral-8x7b-32768\",\n",
        "               groq_api_key=groq_api_key,\n",
        "               temperature=0.3,\n",
        "               max_tokens=32768,\n",
        "               top_p=0.2\n",
        "      )\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVeoMgdMIY5H",
        "outputId": "06cc57a0-d510-42d9-a40b-5e01096ceea3"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py:355: UserWarning: WARNING! top_p is not default parameter.\n",
            "                    top_p was transferred to model_kwargs.\n",
            "                    Please confirm that top_p is what you intended.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7e424cd70d30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7e424cbb45e0>, temperature=0.3, model_kwargs={'top_p': 0.2}, groq_api_key=SecretStr('**********'), max_tokens=32768)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for generating the ontology template\n",
        "ontology_prompt = PromptTemplate(\n",
        "    input_variables=[\"pdf_text\", \"tables\"],\n",
        "    template=\"\"\"\n",
        "      Based on the following extracted content from the PDF, including tables and images,\n",
        "      identify key entities (classes), relationships, properties, instances. It should contain\n",
        "      all the classes and its properties and relationships, instances.\n",
        "      Then, generate an ontology in RDF/Turtle format.\n",
        "\n",
        "      PDF text: {pdf_text}\n",
        "      Tables: {tables}\n",
        "\n",
        "      Please generate only the ontology template without mapping specific data points. I dont want you to give any explanation\n",
        "      I want only the ontology template.\n",
        "\n",
        "      eg: RDF/Turtle ontology template:\n",
        "      @prefix ex: <http://example.org/ontology#> .\n",
        "        # Classes\n",
        "        ex:TestingTool a rdfs:Class .\n",
        "        ex:Algorithm a rdfs:Class .\n",
        "        ex:Dataset a rdfs:Class .\n",
        "\n",
        "        # Properties\n",
        "        ex:usesDataset a rdf:Property ;\n",
        "            rdfs:domain ex:TestingTool ;\n",
        "            rdfs:range ex:Dataset .\n",
        "\n",
        "        ex:implementsAlgorithm a rdf:Property ;\n",
        "            rdfs:domain ex:TestingTool ;\n",
        "            rdfs:range ex:Algorithm .\n",
        "\n",
        "        # Example Instances\n",
        "        ex:EvoSuite a ex:TestingTool ;\n",
        "            ex:implementsAlgorithm ex:GeneticAlgorithm ;\n",
        "            ex:usesDataset ex:SF100Corpus .\n",
        "\n",
        "        ex:GeneticAlgorithm a ex:Algorithm .\n",
        "\n",
        "        ex:SF100Corpus a ex:Dataset ;\n",
        "            rdfs:label \"SF100 Corpus\" .\n",
        "\n",
        "            \"\"\"\n",
        ")\n",
        "\n",
        "# Create an LLM chain using the prompt and LLM\n",
        "ontology_chain = ontology_prompt | model"
      ],
      "metadata": {
        "id": "PRC44iyWD6Zc"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def generate_ontology_mixtral(cleaned_data, rate_limit_tpm):\n",
        "    pdf_text = cleaned_data['pdf_text'][:5000]\n",
        "    tables = \"\\n\".join([str(table) for table in cleaned_data['tables'][:5]])\n",
        "\n",
        "    # Calculate time to wait between requests\n",
        "    wait_time = 60 / rate_limit_tpm\n",
        "\n",
        "    # Generate ontology template\n",
        "    ontology_template = ontology_chain.invoke({\n",
        "        \"pdf_text\": pdf_text,\n",
        "        \"tables\": tables\n",
        "    })\n",
        "\n",
        "    # Sleep to respect rate limit\n",
        "    time.sleep(wait_time)\n",
        "\n",
        "    return ontology_template"
      ],
      "metadata": {
        "id": "xdDaoXSSI1QM"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the cleaned data from the PDF stored in 'combined_data'\n",
        "combined_data = {\n",
        "    \"pdf_text\": pdf_text,        # Cleaned text from PDF\n",
        "    \"tables\": cleaned_tables     # Cleaned tables from the PDF\n",
        "}\n",
        "\n",
        "# Generate the ontology template using Azure OpenAI\n",
        "ontology_template = generate_ontology_mixtral(combined_data,rate_limit_tpm=5000)\n",
        "\n",
        "# Print the generated ontology template\n",
        "print(ontology_template.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO-pIckpJGKT",
        "outputId": "d625477e-767f-4180-d484-9ef6ae7c3678"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the RDF/Turtle ontology template based on the provided content:\n",
            "\n",
            "@prefix ex: <http://example.org/ontology#> .\n",
            "\n",
            "# Classes\n",
            "ex:TestingTool a rdfs:Class .\n",
            "ex:Category a rdfs:Class .\n",
            "ex:Journal a rdfs:Class .\n",
            "ex:Conference a rdfs:Class .\n",
            "ex:Approach a rdfs:Class .\n",
            "ex:Activity a rdfs:Class .\n",
            "ex:Dataset a rdfs:Class .\n",
            "ex:Software a rdfs:Class .\n",
            "\n",
            "# Properties\n",
            "ex:assignedToCategory a rdf:Property ;\n",
            "rdfs:domain ex:TestingTool ;\n",
            "rdfs:range ex:Category .\n",
            "\n",
            "ex:publishedIn a rdf:Property ;\n",
            "rdfs:domain ex:ResearchPaper ;\n",
            "rdfs:range ex:Publication .\n",
            "\n",
            "ex:presentedAt a rdf:Property ;\n",
            "rdfs:domain ex:ResearchPaper ;\n",
            "rdfs:range ex:Conference .\n",
            "\n",
            "ex:hasApproach a rdf:Property ;\n",
            "rdfs:domain ex:ResearchPaper ;\n",
            "rdfs:range ex:Approach .\n",
            "\n",
            "ex:hasActivity a rdf:Property ;\n",
            "rdfs:domain ex:Approach ;\n",
            "rdfs:range ex:Activity .\n",
            "\n",
            "ex:usesDataset a rdf:Property ;\n",
            "rdfs:domain ex:Approach ;\n",
            "rdfs:range ex:Dataset .\n",
            "\n",
            "ex:implementedIn a rdf:Property ;\n",
            "rdfs:domain ex:Software ;\n",
            "rdfs:range ex:TestingTool .\n",
            "\n",
            "# Example Instances\n",
            "ex:OpenSource a ex:Category .\n",
            "ex:Academic a ex:Category .\n",
            "ex:Commercial a ex:Category .\n",
            "ex:AcademicAndOpenSource a ex:Category .\n",
            "ex:CommercialAndOpenSource a ex:Category .\n",
            "\n",
            "ex:IEEE a ex:Journal .\n",
            "ex:Springer a ex:Journal .\n",
            "ex:ACM a ex:Journal .\n",
            "ex:Elsevier a ex:Journal .\n",
            "ex:Wiley a ex:Journal .\n",
            "\n",
            "ex:ICST a ex:Conference .\n",
            "ex:ISSTA a ex:Conference .\n",
            "ex:ICSE a ex:Conference .\n",
            "\n",
            "ex:Evosuite a ex:TestingTool ;\n",
            "ex:assignedToCategory ex:OpenSource ;\n",
            "ex:assignedToCategory ex:Academic .\n",
            "\n",
            "ex:GeneticAlgorithm a ex:Approach .\n",
            "ex:RandomTesting a ex:Approach .\n",
            "ex:PartitionTesting a ex:Approach .\n",
            "\n",
            "ex:InformationArtifacts a ex:Activity .\n",
            "ex:GenerationMechanism a ex:Activity .\n",
            "ex:TestCaseValidity a ex:Activity .\n",
            "ex:TestOracleFormation a ex:Activity .\n",
            "\n",
            "ex:SF100Corpus a ex:Dataset .\n",
            "ex:Defects4JRepository a ex:Dataset .\n",
            "ex:Neo4j a ex:Dataset .\n",
            "ex:JSON a ex:Dataset .\n",
            "ex:MochaJS a ex:Dataset .\n",
            "ex:NodeJS a ex:Dataset .\n",
            "\n",
            "ex:JUnit a ex:Software ;\n",
            "ex:implementedIn ex:Evosuite .\n",
            "\n",
            "ex:Selenium a ex:Software .\n",
            "ex:JMeter a ex:Software .\n",
            "ex:LoadRunner a ex:Software .\n",
            "\n",
            "ex:ResearchPaper a owl:Thing .\n",
            "\n",
            "This is just a template and can be further expanded based on the specific data points.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q7sgmGrGJP5N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}